# ----------- Pretraining ARGS---------------------
pretrain_train_path = "./sentence.txt"
pretrain_dev_path = "./sentence.txt"
do_train = True
max_seq_length = 512
do_lower_case = True
learning_rate = 1e-4
num_train_epochs = 20
warmup_proportion = 0.1
no_cuda = False
local_rank = -1
seed = 42
gradient_accumulation_steps = 1
fp16 = False
loss_scale = 0.
bert_config_json = "./config.json"
vocab_file = "./vocab.txt"
output_dir = "outputs"
masked_lm_prob = 0.15
max_predictions_per_seq = 20
# ----------- Training ARGS---------------------
training_learning_rate = 2.5e-4
bert_training_lr = 5e-5
decay = 0.75
decay_step = 1000
training_epoch = 4
train_dir = "train"
save_name = "train"
log_name = "log.txt"
is_val = False
is_use_psdo= True
train_batch_size = 8
eval_batch_size = 32
total_fold_num = 10
current_kfold_num = 1
val_save_dir = "./val"
dropout_num = 2
multi_dropout = False
is_full_data = False
